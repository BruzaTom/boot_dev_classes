                Welcome to Algorithms


The word "algorithms" was designed to scare young CS students into eating their vegetables. After this course, you'll come to see there is nothing to fear.


Goals of this course

Learn to think algorithmically. Break problems down into easier to solve parts.
Learn about performance optimization. Make your code run faster and more efficiently, even with more data.
Practice solving hard problems. There's no way around it, if you want to be a great developer, you need to solve hard problems.

A word of warning

Don't worry about memorizing this stuff. In particular, don't worry about memorizing the algorithms themselves. My philosophy is that it's mostly a waste of time to memorize anything that's a Google search away.

Instead, focus on understanding how the algorithms work in the moment. You should understand what your code is doing and why - but that doesn't mean you need to memorize the code itself.

Good luck.



                Find Minimum


An "algorithm" is just a set of instructions that can be carried out to solve a problem.

People use algorithms all the time without even realizing it. Practically every function you write in code is an algorithm (well, kinda), even if it's a simple one.

Assignment
In this course, we'll be building pieces of a pretend product: Socialytics - The tool you need to track your growth as an Instagram influencer. We need to show our users the accounts they follow with the lowest follower counts. This will help them know who they follow that isn't popular enough to be worth following anymore.

Implement the "find minimum" algorithm in Python by completing the find_minimum() function:

Set minimum to positive infinity: float("inf").
For each number in the list nums, compare it to minimum. If num is smaller, set minimum to num.
minimum is now set to the smallest number in the list.

Tip
Don't forget to account for edge cases in your function! Check the test code if you're not sure what to expect.


                What is an Algorithm?


In the context of computer science, an algorithm is a finite sequence of well-defined, computer-implementable instructions. In short, an algorithm is:

Defined: there is a specific sequence of steps that performs a task
Unambiguous: there is a "correct" and "incorrect" interpretation of the steps
Implementable: it can be executed using software and hardware

Algorithms are usually written in "pseudocode" (pronounced "sue-dough-code") because an algorithm is a higher-level description of a solution to a problem. An algorithm doesn't care if it's implemented in Python, Go, TypeScript, or (gasp) Java. Pseudocode is just plain English that describes the steps of the algorithm.

Here's some pseudocode for a mystery algorithm:

Start with an original string called S and a new empty string called R.
Loop through S from its last character to its first character, and for each one, add it to the end of R.
Once you’ve processed all the characters, return R.

                Simple Algorithms


If you've heard (usually exaggerated) stories about Leetcode and whiteboarding interviews, you might hear the word "algorithm" and think it's synonymous with "hard problem." That's really not the case, and believing it will only psych you out.

We suffer more often in imagination than in reality

-- Seneca

Algorithms, like anything else, can be understood by breaking them down piece by piece. Take a look at the following algorithm for adding two numbers--it's dead simple:

Start with input variables a and b
Add a and b using the + operator, and assign the result to a new variable, sum
Return the sum variable

Assignment
In Socialytics, we need to calculate the total reach of a group of influencers to estimate how many views a post will get if they all share it.

Complete the sum function. It's a slightly modified version of the algorithm above. Instead of just two numbers, a and b, it accepts a list of numbers and returns the sum of all of them.

Note: The sum of an empty list should be 0.


                Average


We now need a way to show our Instagram influencers the average follower count of the people they follow. This will help them know if they're following people who are more or less popular than them.

Assignment
Complete the average_followers function. It should return the average of the given list of numbers.


                Median


A median is often more useful than an average when the data set contains outliers. For example, if you want to know the typical salary of a software engineer, the median can be a better measure than the average, because the average can be skewed by a few people who make a lot of money.

At Socialytics, we want to show our influencers the median follower count of the people they follow.

Assignment
Complete the median_followers() function to find the median follower count of the given list of numbers.

Order matters - You'll probably want to use the sorted() function to help you out.



                Exponents


I promise we'll get to how this relates to coding, but first we need to review some math stuff.

An exponent indicates how many times a number is to be multiplied by itself.

For example:

53 = 5 * 5 * 5 = 125

Sometimes exponents are also written using the caret symbol (^):

5^3 = 53

Exponent syntax
The ** operator calculates an exponent in Python. (Why not the ^ operator? Blame Fortran.)

square = 2 ** 2
# square = 4

cube = 2 ** 3
# cube = 8

Assignment
In the social media industry, there is a concept called "spread": how much a post spreads due to "reshares" after all of the original author's followers see it. As it turns out, social media posts spread at an exponential rate! We've found that the estimated spread of a post can be calculated with this formula:

estimated_spread = average_audience_followers * ( num_followers ^ 1.2 )

In the formula above, average_audience_followers is an average calculated from each number within the audiences_followers argument - which is a list containing the individual follower counts of the author's followers. For example, if audiences_followers = [2, 3, 2, 19], then:

the author has 4 total followers
each follower has their own 2, 3, 2, and 19 followers, respectively.

Complete the get_estimated_spread function by implementing the formula above. The only input is audiences_followers, which is a list of the follower counts of all the followers the author has. Return the estimated spread. If the audiences_followers list is empty, return 0.


                Exponents Grow


Exponents grow very large very quickly. Let's take a look at an example of them doing that, in code.

Assignment
While the influencers who use our platform are really great at taking selfies, most aren't super great at math. We need to write a tool that predicts an influencer's follower growth over time.

Complete the get_follower_prediction function.

"fitness" influencers: follower count quadruples each month
"cosmetic" influencers: follower count triples each month
All other influencers: follower count doubles each month

For example, if a fitness influencer starts with 10 followers, then after 1 month they should have 40 followers. After 2 months, they would have 160 followers; etc.

Use a geometric sequence formula that's slightly modified for this problem: total = a1 * r^n


                Non-Linear Growth


Exponents are important to understand when it comes to the execution speed of an algorithm. If the number of operations grows quickly as the amount of input data increases, the algorithm will become slower and slower.

Linear	        Quadratic
2 * 2 = 4	    2 ** 2 = 4
3 * 2 = 6	    3 ** 2 = 9
4 * 2 = 8	    4 ** 2 = 16
5 * 2 = 10	    5 ** 2 = 25
6 * 2 = 12	    6 ** 2 = 36
7 * 2 = 14	    7 ** 2 = 49
8 * 2 = 16	    8 ** 2 = 64
9 * 2 = 18	    9 ** 2 = 81

The doubling formula, 2*x, results in linear or straight growth.
The quadratic formula, x^2, keeps growing faster and faster.

What does this have to do with code?
Generally we try to avoid writing code that causes the usage of a resource to grow quadratically (with an exponent).

Sometimes that's a lot of computations (CPU utilization / slowness).
Sometimes that's a lot of memory usage (RAM utilization)
Sometimes that's a large storage requirement (disk space)

A notable exception is in cryptography and security: we want to force attackers to waste resources trying to get at our information.

Notable:
How much larger is 2^64 than 2^32?
2^64 is 2^32 times larger than 2^32


                    Logarithms


A logarithm is the inverse of an exponent.

24 = 16

log216 = 4

"log216" can be read as "log base 2 of 16", and means "the number of times 2 must be multiplied by itself to equal 16".

"log216" might also be written as log2(16)

Some more examples:

Logarithm	Result
log22	        1
log24	        2
log28	        3
log216	        4
log1010	        1
log10100	    2
log101000	    3
log1010000	    4

Python Syntax
There isn't a language-level operator to calculate a logarithm, but we can import the math library and use the math.log() function.

import math

print(f"Logarithm base 2 of 16 is: {math.log(16, 2)}")
# Logarithm base 2 of 16 is: 4.0

Assignment
Let's create an "influencer score". It will be a small number (like less than 100) that will give influencers a metric for comparing their social media success against their peers.

Complete the get_influencer_score function. An influencer score is their engagement percentage multiplied by log base 2 of their follower count.


                Logarithm Quiz

Exponents grow very quickly, and logarithms grow very slowly. A logarithm is the inverse of an exponent.

Generally speaking, it's nice when we can write code that uses log(n) time to run, where n is the amount of data to process. For example, let's say we have a list of 1,000,000 users, and we want to write an algorithm that finds the user with the most followers.

If it takes 1 millisecond to check one user (let's just pretend), a linear algorithm would take 1,000,000 milliseconds, or about 16 minutes and 40 seconds.

A quadratic algorithm (exponent) would take 1,000,000,000,000 milliseconds, or about 31.7 years.

However, a logarithmic algorithm would only take 20 milliseconds! Here's a table to illustrate the difference:


Input Size	Linear (n*2)	Quadratic (n^2)	    Log (log2(n))
10	        20 ms	        100 ms	                3 ms
100	        200 ms	        10,000 ms	            7 ms
1,000	    2,000 ms	    1,000,000 ms	        10 ms
10,000	    20,000 ms	    100,000,000 ms	        14 ms
100,000	    200,000 ms	    10,000,000,000 ms	    17 ms
1,000,000	2,000,000 ms	1,000,000,000,000 ms	20 ms


                Factorials


We're almost done with the math that you'll need for this course! Just one last thing... factorials.

The factorial of a positive integer n, written n!, is the product of all positive integers less than and equal to n.

5! = 5 * 4 * 3 * 2 * 1 = 120

The results of a factorial grow even faster than exponentiation!

n! grows faster than 2^n:

    n!	2^n
2	2	4
3	6	8
4	24	16
5	120	32
6	720	64

Assignment
Influencers need to be able to schedule posts to be published in the future. We've found that the order in which the posts are published drastically affects their performance.

Complete the num_possible_orders function. It takes the number of posts an influencer has in their backlog as input and returns the total number of possible orders in which we could publish the posts.

Tip
Factorials are useful whenever you're trying to count how many ways a collection can be ordered. For example, how many different ways can a deck of cards be arranged?

The first card could be any of the 52 cards.
The second card can be any of the 51 remaining cards.
The third card can be any of the 50 remaining cards, and so on.

That means the total number of possibilities is the 52 options multiplied by 51 options multiplied then by 50 options, and so on.

52 * 51 * 50 ... * 2 * 1

Or in other words, the number of possible combinations for a deck of cards is 52!, or 80,658,175,170,943,878,571,660,636,856,403,766,975,289,505,440,883,277,824,000,000,000,000 (I didn't count the zeros but I think this is correct).

Factorial Quiz
Again, the factorial of a positive integer n, written n!, is the product of all positive integers less than and equal to n.

5! = 5 * 4 * 3 * 2 * 1

Let's visualize how big these things get:

n	n!
2	2
3	6
4	24
5	120
6	720
7	5,040
8	4,0320
9	362,880
10	3,628,800
11	39,916,800
12	479,001,600
13	6,227,020,800
14	87,178,291,200
15	1,307,674,368,000
16	20,922,789,888,000
17	355,687,428,096,000
18	6,402,373,705,728,000
19	121,645,100,408,832,000
20	2,432,902,008,176,640,000


                Exponential Decay


In physics, exponential decay is a process where a quantity decreases over time at a rate proportional to its current value.

We've found that Instagram influencers tend to lose followers similarly. This means that the more followers you have, the faster you lose them.

Assignment
Complete the decayed_followers function.

It calculates the final value of a quantity after a certain time has passed, given its initial value and a rate of decay. Return the remaining followers.

remaining_total = quantity * ( retention_rate ^ time )

The retention_rate is the opposite of fraction_lost_daily. If an influencer lost 0.2 (or 20%) of their followers each day, then the retention rate would be 0.8 (or 80%).

Example
intl_followers = 100

fraction_lost_daily = 0.5

After 0 days: 100 followers

After 1 day: 50 followers

After 2 days: 25 followers

After 3 days: 12.5 followers (rounded down)

Note: This isn't the exact formula shown on Wikipedia, but it's the same idea.


                Logarithmic Scale


In some cases, data can span several orders of magnitude, making it difficult to visualize on a linear scale. A logarithmic scale can help by compressing the data so that it's easier to understand.

For example, at Socialytics we have influencers with follower counts ranging from 1 to 1,000,000,000. If we want to plot the follower count of each influencer on a graph, it would be difficult to see the differences between the smaller follower counts. We can use a logarithmic scale to compress the data so that it's easier to visualize.

Assignment
Write a function log_scale(data, base) that takes a list of positive numbers data, and a logarithmic base, and returns a new list with the logarithm of each number in the original list, using the given base.

You may want to use the math.log() function.

Example:

log_scale([1, 10, 100, 1000], 10)
# Output: [0.0, 1.0, 2.0, 3.0]

log_scale([1, 2, 4, 8], 2)
# Output: [0.0, 1.0, 2.0, 3.0]



                Big O Notation


There are a lot of existing algorithms; some are fast and some are slow. Some use lots of memory. It can be hard to decide which algorithm is the best to solve a particular problem. "Big O" analysis (pronounced "Big Oh", not "Big Zero") is one way to compare the practicality of algorithms.

Big O is a characterization of algorithms according to their worst-case growth rates

We write Big-O notation like this:

O(formula)

Where formula describes how an algorithm's run time or space requirements grow as the input size grows.

see 
https://www.youtube.com/watch?time_continue=649&v=cmahmqABn0g&embeds_referring_euri=https%3A%2F%2Fwww.boot.dev%2F&source_ve_path=MTM5MTE3LDI4NjYzLDI4NjY2

O(1) - constant
O(n) - linear
O(n^2) - squared
O(2^n) - exponential
O(n!) - factorial

The following chart shows the growth rate of several different Big O categories. The size of the input is shown on the x axis and how long the algorithm will take to complete is shown on the y axis.

see(images/big o gaph.jpeg)

As the size of inputs grows, the algorithms become slower to complete (take longer to run). The rate at which they become slower is defined by their Big O category.

For example, O(n) algorithms slow down more slowly than O(n^2) algorithms.

The worst Big-O category?
The algorithms that slow down the fastest in our chart are the factorial and exponential algorithms, or O(n!), and O(2^n).


                O(n) - Order "n"


O(n) is very common - When the number of steps in an algorithm grows at the same rate as its input size, it's classified as O(n)

For example, our find min algorithm from earlier is O(n):

Set min to positive infinity.
For each number in the list, compare it to min. If it is smaller, set min to that number.
min is now set to the smallest number in the list.

The input to the find min algorithm is a list of size n. Because we loop over each item in the input once, we add one step to our algorithm for each item in our list.

As we use find min with larger and larger inputs, the length of time it takes to execute the function grows at a steady linear pace. We can reasonably estimate the time it will take to run, based on a previous measurement. If we find that:

    Input size	     Time to run
find_min(10 items)	    2 ms

Then we can estimate the following:

    Input size	             Time to run
find_min(100 items)	            20 ms
find_min(1000 items)	        200 ms
find_min(10000 items)	        2000 ms

Assignment
At Socialytics we now need to display to our users the people who follow them with the highest engagement count. This will help them know which of their followers they should follow back.

Complete the find_max function. It should take a list of integers and return the largest value in the list.

The "runtime complexity" (aka Big O) of this function should be O(n).


                O(n^2) - Order "n squared"


O(n^2) grows in complexity much more rapidly. That said, for small and medium input sizes, these algorithms can still be very useful.

A common reason an algorithm falls into O(n^2) is by using a nested loop, where the number of iterations of each loop is equal to the number of items in the input:

for person_one in persons:
    for person_two in persons:
        # every combination of people
        # will go on a date... twice!
        go_on_date(person_one, person_two)

Assignment
Socialytics needs search capabilities! For now, we'll build something slow (and frankly awful) so we can see an n^2 algorithm in practice.

Complete the does_name_exist function. It should loop over all of the first/last name combinations in the first_names and last_names lists. If it finds a combination that matches the full_name it should return True. If the full name isn't found, it should return False instead.

                Observe

When you run your completed code, notice how each successive call to does_name_exist takes quite a bit longer. Assuming the length of first_names and last_names is the same, each new name doesn't add n steps to the algorithm it adds n^2 steps.

If does_name_exist(10 names, 10 names) takes just 1 second to complete, then we can estimate:

does_name_exist(100 names, 100 names) = 100 seconds
does_name_exist(1000 names, 1000 names) = 10,000 seconds
does_name_exist(10000 names, 10000 names) = 1,000,000 seconds


                N^2 Quiz


Refer to the following functions, and assume that first_names and last_names are the same length.

def print_names_one(first_names):
    for first_name in first_names:
        print(first_name)

def print_names_two(first_names, last_names):
    for first_name in first_names:
        for last_name in last_names:
            print(first_name, last_name)


                O(nm)


O(nm) is very similar to O(n^2), but instead of a single input that we care about, there are two. If n and m increase at the same rate, then O(nm) is effectively the same as O(n^2). However, if n or m increases faster or slower, then it's useful to track their complexity separately.

Assignment
Socialytics needs a new tool that allows big brands to see how many of an influencer's followers are loyal to their brand. Complete the get_avg_brand_followers function. It takes two inputs:

-all_handles: a 2-dimensional list, or "list of lists" of strings representing instagram user handles on a per-influencer basis.
-brand_name: a string.

get_avg_brand_followers returns the average number of handles that contain the brand_name across all the lists. Each list represents the audience of a single influencer.

Example input/output
Input:

all_handles = [
    ["cosmofan1010", "cosmogirl", "billjane321"],
    ["cosmokiller", "gr8", "cosmojane3"],
    ["iloveboots", "paperthin"]
]
brand_name = "cosmo"

Expected output: 1.33 (handles per influencer, because 4 handles contained "cosmo" and there are 3 lists)

Observe
Regarding Big O, the number of influencers (the number of lists) matters. That's our n. However, the average number of followers of each influencer (the average length of the lists) is just as important. That's our m.


                Constants Don't Matter


Big-O notation only describes the theoretical growth rate of algorithms. It doesn't deal with the actual time an algorithm takes to run on a given machine. As such, when doing Big O analysis, we don't let ourselves get bogged down in details.

For example, take a look at the following functions:

def print_names_once(names):
    for name in names:
        print(name)

def print_names_twice(names):
    for name in names:
        print(name)
    for name in names:
        print(name)

As you would expect, print_names_once will take half the time to run as print_names_twice. And in the real world of software engineering, cutting speed in half is awesome. The funny thing about Big O analysis is that we don't care. We're academics™.

Both of the functions above have the same rate of growth, O(n). You might be tempted to say, "print_names_twice should be O(2 * n)" but you would be missing the whole point of Big O.

In Big O analysis we drop all constants because while they affect the runtime, they don't affect the change in the runtime.

O(2 * n) -> O(n)
O(10 * n^2) -> O(n^2)

see
https://www.youtube.com/watch?v=T0Egtqli4rg


                Order 1


O(1) means that no matter the size of the input, there is no growth in the runtime of the algorithm. This is also referred to as a "constant time" algorithm.

In Python, a dictionary offers the ability to look items up by key, which is an operation that is independent of the size of the dictionary:

# this is a constant time lookup
org = organizations[org_id]

Dictionary lookups are O(1). Which is one of the reasons dictionaries and dictionary-equivalents in other languages are used all over the place.

Assignment
We need to be able to search our Socialytics user base more quickly! Our users are complaining that the search bar is painfully slow. You'll notice that if you run the code in its current state, it will take a very long time.

The find_last_name function takes

-names_dict: a dictionary of first_name -> last_name.
-first_name: a string.

If first_name is a key in the dictionary, find_last_name returns the associated last name. If the key is not found, it returns None.

Write the function so that it runs quickly! It should be O(1).


                Order Log N


O(log(n)) algorithms are only slightly slower than O(1), but much faster than O(n). They do grow according to the input size, n, but only according to the log of the input.

O(n):

n	        time
8	        8 ms
64	        64 ms
1024	    1024 ms
1048576	    1048576 ms

O(log(n)):

n	        time
8	        3 ms
64	        6 ms
1024	    10 ms
1048576	    20 ms

            Binary Search

see
images/binary search.png

A binary search algorithm is a common example of an O(log(n)) algorithm. Binary searches work on a pre-sorted list of elements.

Pseudocode
Given two inputs:

-An array of n elements sorted from least to greatest
-A target value:

Do the following:

-Set low = 0 and high = n - 1.
-While low <= high:
    -Set median (the position of the middle element) to (low + high) // 2, which is the greatest integer less than or equal to (low + high) / 2
    -If array[median] == target, return True
    -Else if array[median] < target, set low to median + 1
    -Otherwise set high to median - 1
-Return False

At each iteration of loop, we halve the list. Which makes the algorithm O(log(n)). In other words, to add one more step to the runtime, we'd have to double the size of the input. Binary searches are fast.

Assignment
We have a popular Instagrammer using our Socialytics app, and she needs to be able to quickly search for posts from a particular day. This function will be the backbone of her search screen.

Complete the binary_search function. It should follow the algorithm as described above.


            Name Count


In Socialytics, we process tons of user's names. They are often structured as lists of lists. For example, a separate list of users for each influencer's followers.

Assignment
Complete the count_names function.

It should iterate over all the names in the nested list_of_lists and count all the instances of target_name, then return the count.

Observe
What's the time complexity of your solution? It should be O(n) on the total number of names, but O(mn) if you consider m to be the number of lists and n to be the average length of a list.


            Remove Duplicate Elements


We're interested in showing the unique follower counts of all of an influencer's followers.

Assignment
Complete the remove_duplicates function. It takes a list of integers nums and returns a new list of integers. The returned list of integers should consist of all the unique follower counts in nums without any duplicates.

We want to preserve the order of the list so be careful when using a set!


                Sorting Algorithms
                

Almost every action you take in a web app relies on sorted data. Just looking up a user's profile in a database likely relies on a sorted index (which we'll talk about in another course).

Fortunately, most programming languages provide their own standard sorting implementation. In Python, for example, we can use the sorted function:

items = [1, 5, 3]
print(sorted(items)) # [1, 3, 5]

Assignment
We need to sort influencers by vanity. Complete the vanity and vanity_sort functions.

The vanity function accepts an instance of an Influencer and returns their vanity score. The vanity score should be the number of links in their bio multiplied by 5, plus their number of selfies. (Links in bio are weighted more heavily)

The vanity_sort function should return a list of influencers, ordered by their vanity from least to greatest. You can pass a function as a named parameter called key to sorted to control the metric the sorting algorithm will use for comparisons.

[personal note]

sorted(iterable, /, *, key=None, reverse=False)
Return a new sorted list from the items in iterable.

Has two optional arguments which must be specified as keyword arguments.

key specifies a function of one argument that is used to extract a comparison key from each element in iterable (for example, key=str.lower). The default value is None (compare the elements directly).

reverse is a boolean value. If set to True, then the list elements are sorted as if each comparison were reversed.


                Bubble Sort


Wait, if the standard sorted() function exists, why should we learn to write a sorting algorithm from scratch?

In all seriousness, in this chapter we'll be building some of the most famous sorting algorithms from scratch because:

-It's good to understand how they work under the hood
-It's great algorithmic thinking practice
-It's fun! (at least for me)

Bubble sort is a very basic sorting algorithm named for the way elements "bubble up" to the top of the list.

see
https://youtu.be/_t-yshdh_ow

Bubble sort repeatedly steps through a slice and compares adjacent elements, swapping them if they are out of order. It continues to loop over the slice until the whole list is completely sorted. Here's the pseudocode:

Set swapping to True
Set end to the length of the input list
While swapping is True:
    Set swapping to False
    For i from the 2nd element to end:
        If the (i-1)th element of the input list is greater than the ith element:
            Swap the (i-1)th element and the ith element
            Set swapping to True
    Decrement end by one
Return the sorted list

Assignment
While our avocado toast influencers were happy with our search functionality, now they want to be able to sort all their followers by follower count. Bubble sort is a straightforward sorting algorithm that we can implement quickly, so let's do that!

Complete the bubble_sort function according to the described algorithm above.


                Bubble Sort Big O


Implementation reference:

def bubble_sort(nums):
    swapping = True
    end = len(nums)
    while swapping:
        swapping = False
        for i in range(1, end):
            if nums[i - 1] > nums[i]:
                temp = nums[i - 1]
                nums[i - 1] = nums[i]
                nums[i] = temp
                swapping = True
        end -= 1
    return nums

            Best and Worst Case

Sometimes it's useful to know how the algorithm will perform based on what the input data is instead of just how much data there is. In the case of bubble sort (and many other algorithms), the best and worst case scenarios can actually change the time complexity.

-Best case: If the data is pre-sorted, bubble sort becomes really fast. Can you see why?
-Worst case: If the data is in reverse order, bubble sort becomes really slow (but still in the same complexity class as random data). Can you see why?

[questions]

What is bubble sort's Big O complexity?
O(n^2)

What are its best and worst case run times?
At best n, at worst n^2


                Why Bubble Sort?


Bubble sort is famous for how easy it is to write and understand.

However, it's one of the slowest sorting algorithms, and as a result is almost never used in practice. That said, we covered it because it's a useful thought exercise so that you can appreciate why the more complex and performant algorithms are better. Let's cover those next.

[question]
What is a good reason to use bubble sort?
Because im studying algorithms


                Merge Sort


Merge sort is a recursive sorting algorithm and it's quite a bit faster than bubble sort. It's a divide and conquer algorithm.:

Divide: divide the large problem into smaller problems, and recursively solve the smaller problems
Conquer: Combine the results of the smaller problems to solve the large problem

In merge sort we:

Divide the array into two (equal) halves (divide)
Recursively sort the two halves
Merge the two halves to form a sorted array (conquer)

Algorithm:
The algorithm consists of two separate functions, merge_sort() and merge().

merge_sort() divides the input array into two halves, calls itself on each half, and then merges the two sorted halves back together in order.

The merge() function merges two already sorted lists back into a single sorted list. At the lowest level of recursion, the two "sorted" lists will each only have one element. Those single element lists will be merged into a sorted list of length two, and we can build from there.

In other words, all the "real" sorting happens in the merge() function.

merge_sort() pseudocode:
Input: A, an unsorted list of integers

If the length of A is less than 2, it's already sorted so return it
Split the input array into two halves down the middle
Call merge_sort() twice, once on each half
Return the result of calling merge(sorted_left_side, sorted_right_side) on the results of the merge_sort() calls

merge() pseudocode
Inputs: A and B. Two sorted lists of integers

Create a new final list of integers.
Set i and j equal to zero. They will be used to keep track of indexes in the input lists (A and B).
Use a loop to iterate over A and B at the same time. If an element in A is less than or equal to its respective element in B, add it to the final list and increment i. Otherwise, add the item in B to the final list and increment j.
After comparing all the items, there may be some items left over in either A or B. Add those extra items to the final list.
Return the final list.

Assignment
Our Socialytics users are complaining that when they sort their followers by follower count, it gets really slow if they have more than 1,000 followers (because we're using Bubble Sort). Let's speed it up for them with merge sort.

Complete the merge_sort and merge functions according to the described algorithms.


                Merge Sort Big O


Merge sort implementation reference:

def merge_sort(nums):
    if len(nums) < 2:
        return nums
    first = merge_sort(nums[: len(nums) // 2])
    second = merge_sort(nums[len(nums) // 2 :])
    return merge(first, second)


def merge(first, second):
    final = []
    i = 0
    j = 0
    while i < len(first) and j < len(second):
        if first[i] <= second[j]:
            final.append(first[i])
            i += 1
        else:
            final.append(second[j])
            j += 1
    while i < len(first):
        final.append(first[i])
        i += 1
    while j < len(second):
        final.append(second[j])
        j += 1
    return final

What is merge sort's Big O complexity?
O(n*log(n))


                Why Merge Sort?


Pros:
Fast: Merge sort is much faster than bubble sort. O(n*log(n)) instead of O(n^2).
Stable: Merge sort is a stable sort which means that values with duplicate keys in the original list will be in the same order in the sorted list.

Cons:
Memory usage: Most sorting algorithms can be performed using a single copy of the original array. Merge sort requires extra subarrays in memory.
Recursive: Merge sort requires many recursive function calls, and in many languages (like Python), this can incur a performance penalty.

When might merge sort be a good idea?
Need a fast sorting algorithm and memory isnt an issue.

Which function is recursive?
merge_sort


                Insertion Sort


Insertion sort builds a sorted list one item at a time. It's much less efficient on large lists than merge sort because it's O(n^2), but it's actually faster (not in Big O terms, but due to smaller constants) than merge sort on small lists.

Assignment
Our influencers want to sort their affiliate deals by revenue. None of our users have more than a couple hundred affiliate deals, so we don't need an n * log(n) algorithm like merge sort. In fact, insertion_sort can be faster than merge_sort, and uses less of our server's memory.

Complete the insertion_sort function according to the given pseudocode:

For each index in the input list:
    Set a j variable to the current index
    While j is greater than 0 and the element at index j-1 is greater than the element at index j:
        Swap the elements at indices j and j-1
        Decrement j by 1
Return the list

Tip
In some languages you need to use a temp variable to swap values, but in python you can do that in a single line:

a = 5
b = 3
a, b = b, a
print(a)
# 3
print(b)
# 5

                Insertion Sort Big O


Insertion sort has a Big O of O(n^2), because that is its worst case complexity.

The outer loop of insertion sort always executes n times, while the inner loop depends on the input.

Best case: If the data is pre-sorted, insertion sort becomes really fast. Can you see why?
Average case: The average case is O(n^2) because the inner loop will execute about half of the time.
Worst case: If the data is in reverse order, it's still O(n^2) and the inner loop will execute every time.

Reference

def insertion_sort(nums):
    for i in range(len(nums)):
        j = i
        while j > 0 and nums[j - 1] > nums[j]:
            nums[j], nums[j - 1] = nums[j - 1], nums[j]
            j -= 1
    return nums

What is insertion sort's Big O complexity on a pre-sorted array?
O(n)


                Why Use Insertion Sort?


Fast: for very small data sets (even faster than merge sort and quick sort, which we'll cover later)
Adaptive: Faster for partially sorted data sets
Stable: Does not change the relative order of elements with equal keys
In-Place: Only requires a constant amount of memory
Online: Can sort a list as it receives it

                Why is insertion sort fast for small lists?

Many production sorting implementations use insertion sort for very small inputs under a certain threshold (very small, like 10-ish), and switch to something like quicksort for larger inputs. They use insertion sort because:

There is no recursion overhead
It has a tiny memory footprint
It's a stable sort as described above

Is insertion sort useful in production systems?
yes but only on very small inputs

Which algorithm uses the least memory, and which is generally faster respectively?
insertion sort, merge sort


            Quick Sort

Quick sort is an efficient sorting algorithm that's widely used in production sorting implementations. Like merge sort, quick sort is a recursive divide and conquer algorithm.

Divide:

Select a pivot element that will preferably end up close to the center of the sorted pack
Move everything onto the "greater than" or "less than" side of the pivot
The pivot is now in its final position
Recursively repeat the operation on both sides of the pivot

Conquer:

The array is sorted after all elements have been through the pivot operation

see https://storage.googleapis.com/qvault-webapp-dynamic-assets/lesson_videos/quick-sort.mp4

    Pseudocode

Select a "pivot" element - We'll arbitrarily choose the last element in the list
Move through all the elements in the list and swap them around until all the numbers less than the pivot are on the left, and the numbers greater than the pivot are on the right
Move the pivot between the two sections where it belongs
recursively repeat for both sections

Assignment:
We now have two sorting algorithms on our Socialytics backend! It is a bit annoying to maintain both in the codebase. Quicksort is fast on large datasets just like merge sort, but is also lighter on memory usage. Let's use quick sort for both follower count and influencer revenue sorting!

Complete the quick_sort and partition functions according to the given algorithms.

Note: The process is started with quick_sort(A, 0, len(A)-1).

quick_sort(nums, low, high):
    If low is less than high:
        Partition the input list using the partition function
        Recursively call quick_sort on the left side of the partition
        Recursively call quick_sort on the right side of the partition
        
partition(nums, low, high):
    Set pivot to the element at index high
    Set i to low
    For each index (j) from low to high
        If the element at index j is less than the pivot:
            Swap the element at index i with the element at index j
            Increment i by 1
        Swap the element at index i with the element at index high
    Return the index i


                Quick Sort Big O


On average, quicksort has a Big O of O(n*log(n)). In the worst case, and assuming we don't take any steps to protect ourselves, it can degrade to O(n^2). partition() has a single for-loop that ranges from the lowest index to the highest index in the array. By itself, the partition() function is O(n). The overall complexity of quicksort is dependent on how many times partition() is called.

Worst case: The input is already sorted. An already sorted array results in the pivot being the largest or smallest element in the partition each time, meaning partition() is called a total of n times.

Best case: The pivot is the middle element of each sublist which results in log(n) calls to partition().

def quick_sort(nums, low, high):
    if low < high:
        p = partition(nums, low, high)
        quick_sort(nums, low, p - 1)
        quick_sort(nums, p + 1, high)


def partition(nums, low, high):
    pivot = nums[high]
    i = low
    for j in range(low, high):
        if nums[j] < pivot:
            nums[i], nums[j] = nums[j], nums[i]
            i += 1
    nums[i], nums[high] = nums[high], nums[i]
    return i

What causes quick sort to break down to O(n^2)?
An already sorted list

What is quick sort's worst case Big O complexity?
O(n^2)


                Fixing Quick Sort


While the version of quicksort that we implemented is almost always able to perform at speeds of O(n*log(n)), its Big O is still technically O(n^2) due to the worst-case scenario. We can fix this by altering the algorithm slightly.

Two of the approaches are:

Shuffle input randomly before sorting. This can trivially be done in O(n) time.
Actively find the median of a sample of data from the partition, this can be done in O(1) time.

        Random Approach

The random approach is easier to code, which is nice if you're the one writing the code.

The function simply shuffles the list into random order before sorting it, which is an O(n) operation. The likelihood of shuffling a large list into sorted order is so low that it's not worth considering.

        Median Approach

Another popular solution is to use the "median of three" approach. Three elements (for example: the first, middle, and last elements) of each partition are chosen and the median is found between them. That item is then used as the pivot.

This approach has less overhead, and also doesn't require randomness to be injected into the function, meaning it can remain deterministic and pure.

What does shuffling quick sort's input do?
Practically ensures a runtime of O(n*log(n))

Why is the median-of-three approach sometimes used?
it dosent require randomness and impurity


                Why Use Quick Sort?


Pros:

Very fast: At least it is in the average case
In-Place: Saves on memory, doesn't need to do a lot of copying and allocating

Cons:

Typically unstable: changes the relative order of elements with equal keys
Recursive: can incur a performance penalty in some implementations
Pivot sensitivity: if the pivot is poorly chosen, it can lead to poor performance

All this said, quicksort is widely used in the real world because the trade-offs are often worth it. For example, it's a default in PostgreSQL, a popular open-source database.

I'd also like to shoutout timsort, which is a hybrid sorting algorithm that uses a combination of merge sort and insertion sort. It's the default sorting algorithm used by the sorted function in Python.

If you need a stable sorting algorithm, which would you use?
merge sort

Which would you be more likely to use in a large production system?
quick sort


                Selection Sort


Another sorting algorithm we never covered in-depth is called "selection sort". It's similar to bubble sort in that it works by repeatedly swapping items in a list. However, it's slightly more efficient than bubble sort because it only makes one swap per iteration.

Selection sort pseudocode:

For each index:
    Set smallest_idx to the current index
    For each index from smallest_idx+1 to the end of the list:
        If the number at the inner index is smaller than the number at smallest_idx
            set smallest_idx to the inner index
    Swap the number at the current index with the number at smallest_idx

Assignment
Complete the selection_sort function.

It should sort the input list nums in ascending order using the selection sort algorithm. The function should then return the sorted list